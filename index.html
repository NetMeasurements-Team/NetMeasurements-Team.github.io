<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Home - Network Measurements</title>
<meta name="description" content="">


  <meta name="author" content="Network Measurements">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Network Measurements">
<meta property="og:title" content="Home">
<meta property="og:url" content="/">


  <meta property="og:description" content="">












<link rel="canonical" href="/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Network Measurements Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<meta name="google-site-verification" content="b5AOEGRxwvM3ll6LBRC222Zk4J8ojJWZ_J_jsuT0nt4" />

    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Network Measurements
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/">Home</a>
            </li><li class="masthead__menu-item">
              <a href="/#-publications">Publications</a>
            </li><li class="masthead__menu-item">
              <a href="/code">Code</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="/">
        <img src="/assets/images/bio-photo.svg" alt="Network Measurements" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="/" itemprop="url">Network Measurements</a>
    </h3>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://netmeasurements-team.github.io/" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://github.com/NetMeasurements-Team" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:netmeasurements.hwp@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="netmeasurements.hwp@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Home">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="/" class="u-url" itemprop="url">Home
</a>
          </h1>
          


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
        <h3 id="-research-interests"><i class="fas fa-flask"> Research interests</i></h3>

<p>Our main research interests are on the performance evaluation, simulation, design and experimentation on networked systems with particular focus on Programmable networks, Traffic generation, and Network Monitoring.</p>

<h3 id="-news"><i class="fas fa-newspaper"></i> News</h3>

<ul style="list-style-type:none;">

  <li> <a href="/2025/01/27/ICLR.html"><b>2025-01:</b> Wang Chao's MITUNE paper accepted at ICLR 2025</a> </li>

  <li> <a href="/2024/01/15/Conext.html"><b>2024-01:</b> DUMBO accepted at Conext 2024</a> </li>

  <li> <a href="/2023/10/18/Conext.html"><b>2023-10:</b> SPADA accepted at Conext 2023</a> </li>

  <li> <a href="/2023/10/16/ConextSW.html"><b>2023-10:</b> Prelimary work on generative data augmentation for traffic classification accepted at Conext SW 2022</a> </li>

  <li> <a href="/2022/12/06/ConextSW.html"><b>2022-12:</b> Prelimary work on learned data structures presented at Conext SW 2022</a> </li>

</ul>

<h3 id="-publications"><i class="fas fa-newspaper"> Publications</i></h3>

<ul class="bibliography"><li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [ICLR 25] </b>
      <div>
        
          <button style="background-color: #4CAF50; border-radius: 8px; "> <b> conference</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <b>Fine-grained Attention in Hierarchical Transformers for Tabular Time-series</b>
         
      </div>
      <div>Wang, Chao and Franzese, Giulio Finamore, Alessandro and Gallo, Massimo and Michiardi, Pietro</div>


      
      <i>In International Conference on Learning Representations 2025</i> 2025

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@inproceedings{wangchao2025ICLR,
  title = {Fine-grained Attention in Hierarchical Transformers for Tabular Time-series},
  author = {Wang, Chao and Franzese, Giulio Finamore, Alessandro and Gallo, Massimo and Michiardi, Pietro},
  year = {2025},
  booktitle = {In International Conference on Learning Representations 2025},
  series = {ICLR 25}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">Diffusion models for Text-to-Image (T2I) conditional generation have recently achieved tremendous success. Yet, aligning these models with user’s intentions still involves a laborious trial-and-error process, and this challenging alignment problem has attracted considerable attention from the research community. In this work, instead of relying on fine-grained linguistic analyses of prompts, human annotation, or auxiliary vision-language models, we use Mutual Information (MI) to guide model alignment. In brief, our method uses self-supervised fine-tuning and relies on a point-wise (MI) estimation between prompts and images to create a synthetic fine-tuning set for improving model alignment. Our analysis indicates that our method is superior to the state-of-the-art, yet it only requires the pre-trained denoising network of the T2I model itself to estimate MI, and a simple fine-tuning strategy that improves alignment while maintaining image quality.</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [CoNEXT 24] </b>
      <div>
        
        <button style="background-color: #008CBA; border-radius: 8px; "> <b> journal</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <b>Taming the Elephants: Affordable Flow Length Prediction in the Data Plane</b>
         
      </div>
      <div>Azorin, Raphael and Monterubbiano, Andrea and Castellano, Gabriele and Gallo, Massimo and Rossi, Dario and Pontarelli, Salvatore</div>


      
        <i>Proceedings of the ACM on Networking</i> 2024

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@article{CoNEXT24,
  author = {Azorin, Raphael and Monterubbiano, Andrea and Castellano, Gabriele and Gallo, Massimo and Rossi, Dario and Pontarelli, Salvatore},
  title = {Taming the Elephants: Affordable Flow Length Prediction in the Data Plane},
  year = {2024},
  journal = {Proceedings of the ACM on Networking},
  topic = {coda},
  series = {CoNEXT 24}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">Machine Learning (ML) shows promising potential for enhancing networking tasks. In particular, early flow size prediction would be beneficial for a wide range of use cases. However, implementing an ML-enabled system is a challenging task due to network devices limited resources. Previous works have demonstrated the feasibility of running simple ML models in the data plane, yet their integration in a practical end-to-end system is not trivial. Additional challenges in resources management and model maintenance need to be addressed to ensure the network task(s) performance improvement justifies the system overhead. In this work, we propose DUMBO, a versatile end-to-end system to generate and exploit flow size hints at line rate.Our system seamlessly integrates and maintains a simple ML model that offers early coarse-grain flow size prediction in the data plane. We evaluate the proposed system on flow scheduling, per-flow packet inter-arrival time distribution, and flow size estimation using real traffic traces, and perform experiments using an FPGA prototype running on an AMD(R)-Xilinx(R) Alveo U280 SmartNIC. Our results show that DUMBO outperforms traditional state-of-the-art approaches by equipping network devices data planes with a lightweight ML model.</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [MILETS 24] </b>
      <div>
        
          <button style="background-color: #4CAF50; border-radius: 8px; "> <b> conference</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <i class="fa fa-file-pdf-o" aria-hidden="true" href="/docs/2024MileTS.pdf"></i> 
          <a href="/docs/2024MileTS.pdf"> <b> Fine-grained Attention in Hierarchical Transformers for Tabular Time-series </b></a> 
         
      </div>
      <div>Raphael, Azorin and Zied, Ben Houidi and Massimo, Gallo and Alessandro, Finamore and Pietro, Michiardi</div>


      
      <i>In KDD Workshop Mining and Learning from Time-Series</i> 2024

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@inproceedings{azorin2024MILETS,
  title = {Fine-grained Attention in Hierarchical Transformers for Tabular Time-series},
  author = {Raphael, Azorin and Zied, Ben Houidi and Massimo, Gallo and Alessandro, Finamore and Pietro, Michiardi},
  year = {2024},
  booktitle = {In KDD Workshop Mining and Learning from Time-Series},
  howpublished = {/docs/2024MileTS.pdf},
  series = {MILETS 24}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">Tabular data is ubiquitous in many real-life systems. In particular, time-dependent tabular data, where rows are chronologically related, is typically used for recording historical events, e.g., financial transactions, healthcare records, or stock history. Recently, hierarchical variants of the attention mechanism of transformer architectures have been used to model tabular time-series data. At first, rows (or columns) are encoded separately by computing attention between their fields. Subsequently, encoded rows (or columns) are attended to one another to model the entire tabular time-series. While efficient, this approach constrains the attention granularity and limits its ability to learn patterns at the field-level across separate rows, or columns. We take a first step to address this gap by proposing Fieldy, a fine-grained hierarchical model that contextualizes fields at both the row and column levels. We compare our proposal against state of the art models on regression and classification tasks using public tabular time-series datasets. Our results show that combining row-wise and column-wise attention improves performance without increasing model size. Code and data are available at this https URL.</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [CoNEXT 23] </b>
      <div>
        
        <button style="background-color: #008CBA; border-radius: 8px; "> <b> journal</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <i class="fa fa-file-pdf-o" aria-hidden="true" href="/docs/2023SPADA.pdf"></i> 
          <a href="/docs/2023SPADA.pdf"> <b> SPADA: A Sparse Approximate Data Structure representation for lightweight per-flow monitoring </b></a> 
         
      </div>
      <div>Monterubbiano, Andrea and Azorin, Raphael and Castellano, Gabriele and Gallo, Massimo and Rossi, Dario and Pontarelli, Salvatore</div>


      
        <i>Proceedings of the ACM on Networking</i> 2023

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@article{CoNEXT23,
  author = {Monterubbiano, Andrea and Azorin, Raphael and Castellano, Gabriele and Gallo, Massimo and Rossi, Dario and Pontarelli, Salvatore},
  title = {SPADA: A Sparse Approximate Data Structure representation for lightweight per-flow monitoring},
  year = {2023},
  journal = {Proceedings of the ACM on Networking},
  howpublished = {/docs/2023SPADA.pdf},
  topic = {coda},
  series = {CoNEXT 23}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">Accurate per-flow monitoring is critical for precise network diagnosis, performance analysis, and network operation and management in general. However, the limited amount of memory available on modern programmable devices and the large number of active flows force practitioners to monitor only the most relevant flows with approximate data structures, limiting their view of network traffic. We argue that, due to the skewed nature of network traffic, such data structures are, in practice, heavily underutilized, i.e., sparse, thus wasting a significant amount of memory. This paper proposes a Sparse Approximate Data Structure (SPADA) representation that leverages sparsity to reduce the memory footprint of per-flow monitoring systems in the data plane while preserving their original accuracy. SPADA representation can be integrated into a generic per-flow monitoring system and is suitable for several measurement use cases. We prototype SPADA in P4 for a commercial FPGA target and test our approach with a custom simulator that we make publicly available, on four real network traces over three different monitoring tasks. Our results show that SPADA achieves 2× to 11× memory footprint reduction with respect to the state-of-the-art while maintaining the same accuracy, or even improving it.</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [AAAI-WS 23] </b>
      <div>
        
          <button style="background-color: #4CAF50; border-radius: 8px; "> <b> conference</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <i class="fa fa-file-pdf-o" aria-hidden="true" href="/docs/2023AAAIWorkshop.pdf"></i> 
          <a href="/docs/2023AAAIWorkshop.pdf"> <b> "It’s a Match!" - A Benchmark of Task Affinity Scores for Joint Learning </b></a> 
         
      </div>
      <div>Azorin, Raphael and Gallo, Massimo and Finamore, Alessandro and Rossi, Dario and Michiardi, Pietro</div>


      
      <i>AAAI - 2nd International Workshop on Practical Deep Learning in the Wild</i> 2023

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@inproceedings{AAAI-23,
  author = {Azorin, Raphael and Gallo, Massimo and Finamore, Alessandro and Rossi, Dario and Michiardi, Pietro},
  title = {"It's a Match!" - A Benchmark of Task Affinity Scores for Joint Learning},
  month = jan,
  year = {2023},
  booktitle = {AAAI - 2nd International Workshop on Practical Deep Learning in the Wild},
  howpublished = {/docs/2023AAAIWorkshop.pdf},
  topic = {representation_learning},
  series = {AAAI-WS 23}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">While the promises of Multi-Task Learning (MTL) are attractive, characterizing the conditions of its success is still an open problem in Deep Learning. Some tasks may benefit from being learned together while others may be detrimental to one another. From a task perspective, grouping cooperative tasks while separating competing tasks is paramount to reap the benefits of MTL, i.e., reducing training and inference costs. Therefore, estimating task affinity for joint learning is a key endeavor. Recent work suggests that the training conditions themselves have a significant impact on the outcomes of MTL. Yet, the literature is lacking of a benchmark to assess the effectiveness of tasks affinity estimation techniques and their relation with actual MTL performance. In this paper, we take a first step in recovering this gap by (i) defining a set of affinity scores by both revisiting contributions from previous literature as well presenting new ones and (ii) benchmarking them on the Taskonomy dataset. Our empirical campaign reveals how, even in a small-scale scenario, task affinity scoring does not correlate well with actual MTL performance. Yet, some metrics can be more indicative than others.</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [PracticalDL 24] </b>
      <div>
        
          <button style="background-color: #4CAF50; border-radius: 8px; "> <b> conference</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <i class="fa fa-file-pdf-o" aria-hidden="true" href="/docs/2023PracticalDL.pdf"></i> 
          <a href="/docs/2023PracticalDL.pdf"> <b> Fine-grained Attention in Hierarchical Transformers for Tabular Time-series </b></a> 
         
      </div>
      <div>Raphael, Azorin and Massimo, Gallo and Alessandro, Finamore and Pietro, Michiardi and Dario, Rossi</div>


      
      <i>In AAAI Workshop on Practical Deep Learning in the Wild</i> 2023

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@inproceedings{azorin2024PracticalDL,
  title = {Fine-grained Attention in Hierarchical Transformers for Tabular Time-series},
  author = {Raphael, Azorin and Massimo, Gallo and Alessandro, Finamore and Pietro, Michiardi and Dario, Rossi},
  year = {2023},
  booktitle = {In AAAI Workshop on Practical Deep Learning in the Wild},
  howpublished = {/docs/2023PracticalDL.pdf},
  series = {PracticalDL 24}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">Tabular data is ubiquitous in many real-life systems. In particular, time-dependent tabular data, where rows are chronologically related, is typically used for recording historical events, e.g., financial transactions, healthcare records, or stock history. Recently, hierarchical variants of the attention mechanism of transformer architectures have been used to model tabular time-series data. At first, rows (or columns) are encoded separately by computing attention between their fields. Subsequently, encoded rows (or columns) are attended to one another to model the entire tabular time-series. While efficient, this approach constrains the attention granularity and limits its ability to learn patterns at the field-level across separate rows, or columns. We take a first step to address this gap by proposing Fieldy, a fine-grained hierarchical model that contextualizes fields at both the row and column levels. We compare our proposal against state of the art models on regression and classification tasks using public tabular time-series datasets. Our results show that combining row-wise and column-wise attention improves performance without increasing model size. Code and data are available at this https URL.</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [CoNEXT-SW 22] </b>
      <div>
        
          <button style="background-color: #4CAF50; border-radius: 8px; "> <b> conference</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <i class="fa fa-file-pdf-o" aria-hidden="true" href="/docs/2022CoNEXT_SW.pdf"></i> 
          <a href="/docs/2022CoNEXT_SW.pdf"> <b> Learned Data Structures for Per-Flow Measurements </b></a> 
         
      </div>
      <div>Monterubbiano, Andrea and Azorin, Raphael and Castellano, Gabriele and Gallo, Massimo and Pontarelli, Salvatore</div>


      
      <i>Proceedings of the 3rd International CoNEXT Student Workshop</i> 2022

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@inproceedings{CoNEXT-22,
  author = {Monterubbiano, Andrea and Azorin, Raphael and Castellano, Gabriele and Gallo, Massimo and Pontarelli, Salvatore},
  title = {Learned Data Structures for Per-Flow Measurements},
  month = dec,
  booktitle = {Proceedings of the 3rd International CoNEXT Student Workshop},
  year = {2022},
  howpublished = {/docs/2022CoNEXT_SW.pdf},
  topic = {coda},
  series = {CoNEXT-SW 22}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">This work presents a generic framework that exploits learning to improve the quality of network measurements. The main idea of this work is to reuse measures collected by the network monitoring tasks to train an ML model that learns some per-flow characteristics and improves the measurement quality re-configuring the memory according to the learned information. We applied this idea to two different monitoring tasks, we identify the main issues related to this approach and we present some preliminary results</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [INFOCOM 22] </b>
      <div>
        
          <button style="background-color: #4CAF50; border-radius: 8px; "> <b> conference</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <i class="fa fa-file-pdf-o" aria-hidden="true" href="/docs/2022Infocom.pdf"></i> 
          <a href="/docs/2022Infocom.pdf"> <b> Accelerating Deep Learning Classification with Error-controlled Approximate-key Caching </b></a> 
         
      </div>
      <div>Finamore, Alessandro and Roberts, James and Gallo, Massimo and Rossi, Dario</div>


      
      <i>Proceedings of IEEE INFOCOM - IEEE Conference on Computer Communications</i> 2022

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@inproceedings{INFOCOM-22,
  title = {Accelerating Deep Learning Classification with Error-controlled Approximate-key Caching},
  author = {Finamore, Alessandro and Roberts, James and Gallo, Massimo and Rossi, Dario},
  year = {2022},
  booktitle = {Proceedings of IEEE INFOCOM - IEEE Conference on Computer Communications},
  month = may,
  topic = {inference},
  howpublished = {/docs/2022Infocom.pdf},
  series = {INFOCOM 22}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">While Deep Learning (DL) technologies are a promising tool to solve networking problems that map to classification tasks, their computational complexity is still too high with respect to real-time traffic measurements requirements. To reduce the DL inference cost, we propose a novel caching paradigm, that we named approximate-key caching, which returns approximate results for lookups of selected input based on cached DL inference results. While approximate cache hits alleviate DL inference workload and increase the system throughput, they however introduce an approximation error. As such, we couple approximate-key caching with an error-correction principled algorithm, that we named auto-refresh.  We analytically model our caching system performance for classic LRU and ideal caches, we perform a trace-driven evaluation of the expected performance, and we compare the benefits of our proposed approach with the state-of-the-art similarity caching – testifying the practical interest of our proposal.</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [HotNets 22] </b>
      <div>
        
          <button style="background-color: #4CAF50; border-radius: 8px; "> <b> conference</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <i class="fa fa-file-pdf-o" aria-hidden="true" href="/docs/2022HotNets.pdf"></i> 
          <a href="/docs/2022HotNets.pdf"> <b> Towards a systematic multi-modal representation learning for network data </b></a> 
         
      </div>
      <div>Zied, Ben Houidi and Raphael, Azorin and Massimo, Gallo and Alessandro, Finamore and Dario, Rossi</div>


      
      <i>Proceedings of the 21st ACM Workshop on Hot Topics in Networks</i> 2022

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@inproceedings{HotNets-22,
  author = {Zied, Ben Houidi and Raphael, Azorin and Massimo, Gallo and Alessandro, Finamore and Dario, Rossi},
  title = {Towards a systematic multi-modal representation learning for network data},
  month = nov,
  booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks},
  year = {2022},
  howpublished = {/docs/2022HotNets.pdf},
  topic = {representation_learning},
  series = {HotNets 22}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">Learning the right representations from complex input data is the key ability of successful machine learning (ML) models. The latter are often tailored to a specific data modality. For example, recurrent neural networks (RNNs) were designed having the processing of sequential data in mind, while convolutional neural networks (CNNs) were designed to exploit spatial correlation naturally present in images. Unlike computer vision (CV) and natural language processing (NLP), each of which targets a single well-defined modality, network ML problems often have a mixture of data modalities as input. Yet, instead of exploiting such abundance, practitioners tend to rely on sub-features thereof, reducing the problem on single modality for the sake of simplicity. In this paper, we advocate for exploiting all the modalities naturally present in network data. As a first step, we observe that network data systematically exhibits a mixture of quantities (e.g., measurements), and entities (e.g., IP addresses, names, etc.). Whereas the former are generally well exploited, the latter are often underused or poorly represented (e.g., with one-hot encoding). We propose to systematically leverage state of the art embedding techniques to learn entity representations, whenever significant sequences of such entities are historically observed. Through two diverse usecases, we show that such entity encoding can benefit and naturally augment classic quantity-based features.</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [CoNEXT-SW 21] </b>
      <div>
        
          <button style="background-color: #4CAF50; border-radius: 8px; "> <b> conference</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <i class="fa fa-file-pdf-o" aria-hidden="true" href="/docs/2021Conext.pdf"></i> 
          <a href="/docs/2021Conext.pdf"> <b> Towards a Generic Deep Learning Pipeline for Traffic Measurements </b></a> 
         
      </div>
      <div>Azorin, Raphaël and Gallo, Massimo and Finamore, Alessandro and Maurizio, Filippone and Pietro, Michiardi and Rossi, Dario</div>


      
      <i>Proceedings of the 2nd International CoNEXT Student Workshop</i> 2021

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@inproceedings{CoNEXT-21,
  author = {Azorin, Raphaël and Gallo, Massimo and Finamore, Alessandro and Maurizio, Filippone and Pietro, Michiardi and Rossi, Dario},
  title = {Towards a Generic Deep Learning Pipeline for Traffic Measurements},
  year = {2021},
  booktitle = {Proceedings of the 2nd International CoNEXT Student Workshop},
  month = dec,
  howpublished = {/docs/2021Conext.pdf},
  topic = {representation_learning},
  series = {CoNEXT-SW 21}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">As networks grow bigger, traffic measurements become more and more challenging. Common practices require specialized solutions tied to specific measurements. We aim at automating the design of generic top-down measurements tools thanks to Deep Learning. To this end, we focus our study on (i) researching an appropriate input traffic representation and (ii) comparing Deep Learning pipelines for several measurements. In this paper, we propose an empirical campaign to study a variety of modeling approaches for multiple traffic metrics predictions, with a strong focus on the trade-off between performance and cost that these approaches offer.</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [SEC 21] </b>
      <div>
        
          <button style="background-color: #4CAF50; border-radius: 8px; "> <b> conference</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <i class="fa fa-file-pdf-o" aria-hidden="true" href="/docs/2021sec.pdf"></i> 
          <a href="/docs/2021sec.pdf"> <b> FENXI: Deep-learning Traffic Analytics at the edge </b></a> 
         
      </div>
      <div>Gallo, Massimo and Finamore, Alessandro and Simon, Gwendal and Rossi, Dario</div>


      
      <i>Proceedings of ACM/IEEE Symposium on Edge Computing</i> 2021

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@inproceedings{SEC-21,
  author = {Gallo, Massimo and Finamore, Alessandro and Simon, Gwendal and Rossi, Dario},
  title = {FENXI: Deep-learning Traffic Analytics at the edge},
  year = {2021},
  booktitle = {Proceedings of ACM/IEEE Symposium on Edge Computing},
  venue = {San Jose},
  month = dec,
  howpublished = {/docs/2021sec.pdf},
  topic = {inference},
  series = {SEC 21}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">Live traffic analysis at the first aggregation point in the ISP network enables the implementation of complex traffic engineering policies but is limited by the scarce processing capabilities, especially for Deep Learning (DL) based analytics. The introduction of specialized hardware accelerators i.e., Tensor Processing Unit (TPU), offers the opportunity to enhance processing capabilities of network devices at the edge. Yet, to date, no packet processing pipeline is capable of offering DL-based analysis capabilities in the data-plane, without interfering with network operations. In this paper, we present FENXI, a system to run complex analytics by leveraging TPU. The design of FENXI decouples forwarding operations and traffic analytics which operates at different granularities i.e., packet and flow levels. We conceive two independent modules that asynchronously communicate to exchange network data and analytics results, and design data structures to extract flow level statistics without impacting per-packet processing. We prototyped and evaluated FENXI on general-purpose servers considering both both adversarial and realistic network conditions. Our analysis shows that FENXI can sustains 100 Gbps line rate traffic processing requiring only limited resources, while also dynamically adapting to variable network conditions</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li>
<li>
<table>
  <tr>
    <th style="text-align: center; width: 150px">
      <b> [SIGCOMM-WS 20] </b>
      <div>
        
          <button style="background-color: #4CAF50; border-radius: 8px; "> <b> conference</b></button>

         
      </div>
    </th>
    <td>
      <div>
        
          <i class="fa fa-file-pdf-o" aria-hidden="true" href="/docs/2020sigcomm.pdf"></i> 
          <a href="/docs/2020sigcomm.pdf"> <b> Real-time Deep Learning based Traffic Analytics </b></a> 
         
      </div>
      <div>Gallo, Massimo and Finamore, Alessandro and Simon, Gwendal and Rossi, Dario</div>


      
      <i>Proceedings of the SIGCOMM Poster and Demo Sessions</i> 2020

       
      
      <div>

        <details name="det" style="display: inline-block; ">
          <summary><i class="fa fa-book" aria-hidden="true"></i>  Bibtex</summary>
          <div class="content"><p style="font-size: 1.1em"><pre>@inproceedings{SIGCOMM-20,
  author = {Gallo, Massimo and Finamore, Alessandro and Simon, Gwendal and Rossi, Dario},
  title = {Real-time Deep Learning based Traffic Analytics},
  year = {2020},
  booktitle = {Proceedings of the SIGCOMM Poster and Demo Sessions},
  venue = {Online},
  month = aug,
  howpublished = {/docs/2020sigcomm.pdf},
  topic = {inference},
  series = {SIGCOMM-WS 20}
}
</pre></p></div>
        </details> 
        

            <details name="det" style="display: inline-block;">
              <summary><i class="fa fa-comment" aria-hidden="true"></i> Abstract</summary>
              <div class="content"><p style="font-size: 1.1em">The increased interest towards Deep Learning technologies has led to the development of a new generation of specialized hardware accelerators, including edge TPUs, which are expected to yield a higher ratio of operations per watt footprint than GPUs. In this demo we investigate the performance of edge TPU for traffic analytics. We sketch the design of a real-time DL traffic classification system running a state-of-the art CNN, and compare inference speed on different hardware (CPU, GPU, TPU).We run stress tests based on synthetic traffic and under different conditions, and collect the results into a dashboard enabling both data exploration and to launch synthetic live tests on top of Ascend 310 TPUs.</p></div>
            </details>
        
      </div>
    </td>
  </tr>
</table>
</li></ul>

<script>
  dets=document.getElementsByName("det");
  console.log(5+1);
  for(let i = 0; i < dets.length; i++) {
    dets[i].addEventListener("toggle", (event) => {
      if (dets[i].open) {
        dets[i].style.display = "block";
      }
      else{
        dets[i].style.display = "inline-block"; 
      }
    });
   }
</script>


        
      </section>

      <footer class="page__meta">
        
        


        

      </footer>

      

      
    </div>

    
  </article>

  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://netmeasurements-team.github.io/" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i> Website</a></li>
        
      
        
          <li><a href="https://github.com/NetMeasurements-Team" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2026 Network Measurements. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>